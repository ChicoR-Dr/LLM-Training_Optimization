python training.py 
Map: 100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1412.34 examples/s]
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
[2025-07-29 14:42:11,755] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-29 14:42:12,835] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|                                                                                                    | 0/375 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 11.5736, 'grad_norm': 10.104419708251953, 'learning_rate': 0.0001968, 'epoch': 0.08}                                    
{'loss': 3.7137, 'grad_norm': 4.5744452476501465, 'learning_rate': 0.0001914666666666667, 'epoch': 0.16}                         
{'loss': 0.4391, 'grad_norm': 0.5312793850898743, 'learning_rate': 0.00018613333333333335, 'epoch': 0.24}                        
{'loss': 0.3406, 'grad_norm': 0.1631845384836197, 'learning_rate': 0.0001808, 'epoch': 0.32}                                     
{'loss': 0.2771, 'grad_norm': 0.10621778666973114, 'learning_rate': 0.00017546666666666666, 'epoch': 0.4}                        
{'loss': 0.264, 'grad_norm': 0.07778987288475037, 'learning_rate': 0.00017013333333333334, 'epoch': 0.48}                        
{'loss': 0.2498, 'grad_norm': 0.0680108591914177, 'learning_rate': 0.0001648, 'epoch': 0.56}                                     
{'loss': 0.2469, 'grad_norm': 0.061670053750276566, 'learning_rate': 0.00015946666666666668, 'epoch': 0.64}                      
{'loss': 0.2852, 'grad_norm': 0.07940678298473358, 'learning_rate': 0.00015413333333333336, 'epoch': 0.72}                       
{'loss': 0.2515, 'grad_norm': 0.07782382518053055, 'learning_rate': 0.0001488, 'epoch': 0.8}                                     
{'loss': 0.231, 'grad_norm': 0.07708258181810379, 'learning_rate': 0.0001434666666666667, 'epoch': 0.88}                         
{'loss': 0.2316, 'grad_norm': 0.07483680546283722, 'learning_rate': 0.00013813333333333335, 'epoch': 0.96}                       
 33%|██████████████████████████████                                                            | 125/375 [07:47<16:03,  3.85s/it]/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.2247, 'grad_norm': 0.053088169544935226, 'learning_rate': 0.0001328, 'epoch': 1.04}                                   
{'loss': 0.2556, 'grad_norm': 0.06846556067466736, 'learning_rate': 0.00012746666666666666, 'epoch': 1.12}                       
{'loss': 0.2321, 'grad_norm': 0.08790338039398193, 'learning_rate': 0.00012213333333333334, 'epoch': 1.2}                        
{'loss': 0.2227, 'grad_norm': 0.05769933760166168, 'learning_rate': 0.00011679999999999999, 'epoch': 1.28}                       
{'loss': 0.2124, 'grad_norm': 0.061696507036685944, 'learning_rate': 0.00011146666666666667, 'epoch': 1.36}                      
{'loss': 0.2341, 'grad_norm': 0.056771405041217804, 'learning_rate': 0.00010613333333333333, 'epoch': 1.44}                      
{'loss': 0.2185, 'grad_norm': 0.07127232104539871, 'learning_rate': 0.00010080000000000001, 'epoch': 1.52}                       
{'loss': 0.2895, 'grad_norm': 0.07573018968105316, 'learning_rate': 9.546666666666667e-05, 'epoch': 1.6}                         
{'loss': 0.211, 'grad_norm': 0.05930480360984802, 'learning_rate': 9.013333333333333e-05, 'epoch': 1.68}                         
{'loss': 0.25, 'grad_norm': 0.055087633430957794, 'learning_rate': 8.48e-05, 'epoch': 1.76}                                      
{'loss': 0.2192, 'grad_norm': 0.08514922112226486, 'learning_rate': 7.946666666666667e-05, 'epoch': 1.84}                        
{'loss': 0.2222, 'grad_norm': 0.07283159345388412, 'learning_rate': 7.413333333333334e-05, 'epoch': 1.92}                        
{'loss': 0.2411, 'grad_norm': 0.06123202294111252, 'learning_rate': 6.879999999999999e-05, 'epoch': 2.0}                         
 67%|████████████████████████████████████████████████████████████                              | 250/375 [15:40<07:52,  3.78s/it]/home/chinmay/Desktop/Cognisynlabs/Products/Simple_chatbot_deployed/chatbot_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'loss': 0.2233, 'grad_norm': 0.06824533641338348, 'learning_rate': 6.346666666666667e-05, 'epoch': 2.08}                        
{'loss': 0.2099, 'grad_norm': 0.058082327246665955, 'learning_rate': 5.813333333333334e-05, 'epoch': 2.16}                       
{'loss': 0.22, 'grad_norm': 0.06252887845039368, 'learning_rate': 5.28e-05, 'epoch': 2.24}                                       
{'loss': 0.2433, 'grad_norm': 0.07207050174474716, 'learning_rate': 4.746666666666667e-05, 'epoch': 2.32}                        
{'loss': 0.217, 'grad_norm': 0.05818409100174904, 'learning_rate': 4.213333333333334e-05, 'epoch': 2.4}                          
{'loss': 0.2228, 'grad_norm': 0.06469026207923889, 'learning_rate': 3.68e-05, 'epoch': 2.48}                                     
{'loss': 0.2287, 'grad_norm': 0.06943877786397934, 'learning_rate': 3.146666666666667e-05, 'epoch': 2.56}                        
{'loss': 0.2411, 'grad_norm': 0.06788657605648041, 'learning_rate': 2.6133333333333333e-05, 'epoch': 2.64}                       
{'loss': 0.2599, 'grad_norm': 0.06887736171483994, 'learning_rate': 2.08e-05, 'epoch': 2.72}                                     
{'loss': 0.2413, 'grad_norm': 0.08494652062654495, 'learning_rate': 1.546666666666667e-05, 'epoch': 2.8}                         
{'loss': 0.2285, 'grad_norm': 0.07132471352815628, 'learning_rate': 1.0133333333333333e-05, 'epoch': 2.88}                       
{'loss': 0.2109, 'grad_norm': 0.0709729865193367, 'learning_rate': 4.800000000000001e-06, 'epoch': 2.96}                         
{'train_runtime': 1415.6363, 'train_samples_per_second': 2.119, 'train_steps_per_second': 0.265, 'train_loss': 0.6394838822682699, 'epoch': 3.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████| 375/375 [23:35<00:00,  3.78s/it]
[rank0]:[W729 15:05:51.035018398 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
