python training_unsloth.py 
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 07-29 19:39:13 [__init__.py:244] Automatically detected platform cuda.
==((====))==  Unsloth 2025.7.11: Fast Llama patching. Transformers: 4.53.2. vLLM: 0.9.2.
   \\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.698 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
../models/TinyLlama-1.1B-Chat-v1.0/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6 does not have a padding token! Will use pad_token = <unk>.
Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.
Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.
Unsloth 2025.7.11 patched 22 layers with 0 QKV layers, 0 O layers and 0 MLP layers.
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1486.23 examples/s]
[2025-07-29 19:39:23,158] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-07-29 19:39:23,523] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 2 | Total steps = 100
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16
 "-____-"     Trainable parameters = 1,126,400 of 1,101,174,784 (0.10% trained)
  0%|                                                                                                    | 0/100 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.866, 'grad_norm': 0.7237986922264099, 'learning_rate': 0.00018, 'epoch': 0.16}                                        
{'loss': 1.7824, 'grad_norm': 1.0696427822113037, 'learning_rate': 0.00018, 'epoch': 0.32}                                       
{'loss': 1.5073, 'grad_norm': 0.6089236736297607, 'learning_rate': 0.0001577777777777778, 'epoch': 0.48}                         
{'loss': 1.4774, 'grad_norm': 0.7925769686698914, 'learning_rate': 0.00013555555555555556, 'epoch': 0.64}                        
{'loss': 1.5044, 'grad_norm': 0.7458349466323853, 'learning_rate': 0.00011333333333333334, 'epoch': 0.8}                         
{'loss': 1.4069, 'grad_norm': 0.8208646178245544, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.96}                         
{'loss': 1.4088, 'grad_norm': 0.7277783155441284, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.11}                          
{'loss': 1.3493, 'grad_norm': 0.9352724552154541, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.27}                         
{'loss': 1.3313, 'grad_norm': 0.6990970969200134, 'learning_rate': 2.4444444444444445e-05, 'epoch': 1.43}                        
{'loss': 1.3695, 'grad_norm': 0.5753133296966553, 'learning_rate': 2.2222222222222225e-06, 'epoch': 1.59}                        
{'train_runtime': 197.8222, 'train_samples_per_second': 8.088, 'train_steps_per_second': 0.506, 'train_loss': 1.5003233337402344, 'epoch': 1.59}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:17<00:00,  1.98s/it]

